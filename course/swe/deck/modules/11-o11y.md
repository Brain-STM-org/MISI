---
theme: default
title: "Module 11: O11y - Understanding Your Code"
routerMode: hash
info: |
  SWE Fundamentals - MISI
  Module 11: O11y (Observability)
drawings:
  persist: false
transition: slide-left
---

# Module 11

## O11y: Understanding Your Code

<div class="pt-8 text-xl text-gray-400">
  SWE Fundamentals • MISI
</div>

<!--
Observability helps you understand what your code is doing.
Essential for debugging production issues.

Estimated time: 25 minutes
Delivery: Just-in-time when teams deploy
-->

---

# Wait, "O11y"?

**Observability** is an 11-letter word.

Developers shortened it: **o11y**

<v-click>

(First letter + count of middle letters + last letter)

</v-click>

<v-click>

Same pattern:
- internationalization → i18n
- localization → l10n
- Kubernetes → k8s

</v-click>

<v-click>

Now you're in on the joke.

</v-click>

<!--
Developer naming conventions can be quirky.
This is a common abbreviation pattern.
-->

---

# Opening Quote

<div class="text-2xl italic text-gray-300 mt-12">
"You can't fix what you can't see."
</div>

<!--
Observability makes the invisible visible.
You need to see problems to solve them.
-->

---

# Learning Objectives

By the end of this module, you will be able to:

<v-clicks>

1. **Explain** what observability means
2. **Distinguish** between logs, metrics, and traces
3. **Write** useful log messages
4. **Read** logs to understand what happened
5. **Understand** what dashboards show

</v-clicks>

<!--
Focus on reading and understanding.
Writing good logs is the practical skill.
-->

---
layout: section
---

# The Problem

Something went wrong. Now what?

---

# The Debugging Challenge

Your code runs on a server. Something goes wrong. A user reports:

<v-click>

*"It's slow"* or *"I got an error"*

</v-click>

<v-click>

**Now what?**

You can't add a `print()` and re-run. The moment is gone.

</v-click>

<v-click>

<div class="mt-4 p-4 bg-blue-900 rounded">
<strong>Observability</strong>: Understanding the internal state of a system by examining its outputs.
</div>

</v-click>

<!--
Production debugging is different from local debugging.
You need to see what happened after the fact.
-->

---

# The Three Pillars

| Pillar | What It Is | Question It Answers |
|--------|------------|---------------------|
| **Logs** | Text records of events | "What happened?" |
| **Metrics** | Numbers over time | "How is it performing?" |
| **Traces** | Request journeys | "Where did time go?" |

<!--
Three complementary approaches to visibility.
Logs are most common for small projects.
-->

---
layout: section
---

# Logs

The story of what happened

---

# What a Log Looks Like

```
2024-01-15T14:23:45Z INFO  User alex@example.com logged in
2024-01-15T14:23:46Z DEBUG Loading user preferences
2024-01-15T14:23:47Z INFO  Fetching dashboard data
2024-01-15T14:23:52Z WARN  Dashboard query took 5s (threshold: 2s)
2024-01-15T14:23:53Z ERROR Failed to load recommendations: Connection refused
```

<v-click>

Each line tells you:
- **When** it happened (timestamp)
- **Severity** (INFO, WARN, ERROR)
- **What** occurred (the message)

</v-click>

<!--
Logs are timestamped records.
They tell the story of execution.
-->

---

# Log Levels

| Level | When to Use |
|-------|-------------|
| **DEBUG** | Detailed diagnostic info |
| **INFO** | Normal operations |
| **WARN** | Potential problems |
| **ERROR** | Something failed |
| **FATAL** | System can't continue |

<v-click>

<div class="mt-4 text-gray-400">
Production: INFO and above (DEBUG is too verbose)<br>
Debugging: Enable DEBUG temporarily
</div>

</v-click>

<!--
Different levels for different purposes.
Filter by level to find what matters.
-->

---

# Bad Logs vs Good Logs

<div class="grid grid-cols-2 gap-8">

<div>

**Bad:**
```python
logger.info("Error")
logger.info("Done")
logger.info("Here")
```

</div>

<div>

**Good:**
```python
logger.info(f"User {user_id} "
  "started checkout")
logger.error(f"Payment failed "
  "for order {order_id}: "
  "{error_message}")
```

</div>

</div>

<v-click>

<div class="mt-4">

Good logs include:
- **Context**: What entity? What IDs?
- **Specifics**: What values? What state?
- **Actionable info**: Can someone investigate?

</div>

</v-click>

<!--
Vague logs are useless logs.
Include enough context to investigate.
-->

---

# Reading Logs

When debugging:

<v-clicks>

1. **Find the timeframe**: When did the problem occur?
2. **Filter by severity**: Start with ERRORs and WARNs
3. **Correlate IDs**: Follow a specific request or user
4. **Look for patterns**: Did this happen before?

</v-clicks>

<v-click>

```bash
# Finding errors at a specific time
grep "ERROR" app.log | grep "2024-01-15T14"

# Following one user's journey
grep "user_id=12345" app.log
```

</v-click>

<!--
Systematic approach to log investigation.
IDs let you trace specific requests.
-->

---
layout: section
---

# Metrics

Numbers over time

---

# What Metrics Show

```
requests_per_second: 250
response_time_p95: 180ms
error_rate: 0.02%
cpu_usage: 45%
memory_usage: 2.3GB
```

<v-click>

Unlike logs (discrete events), metrics track **continuous state**.

</v-click>

<v-click>

| Metric | Why It Matters |
|--------|----------------|
| Request rate | Traffic patterns, capacity |
| Error rate | System health |
| Response time | User experience |
| CPU/Memory | Resource limits |

</v-click>

<!--
Metrics show trends over time.
Essential for understanding system health.
-->

---

# Why Percentiles Matter

*"Average response time: 100ms"*

<v-click>

Sounds good. But averages hide problems:

- 95% of requests: 50ms ✓
- 5% of requests: 2000ms ✗

Average: ~150ms. But 5% of users are suffering.

</v-click>

<v-click>

<div class="mt-4 p-4 bg-yellow-900 rounded">
<strong>Percentiles reveal outlier pain:</strong><br>
• p50: Half are faster (median)<br>
• p95: 95% are faster; 5% are slower<br>
• p99: 99% are faster; 1% are slower
</div>

</v-click>

<!--
Averages can be deceiving.
Percentiles show the experience of unhappy users.
-->

---
layout: section
---

# Dashboards

Making sense of it all

---

# What Dashboards Show

```
┌───────────────────────────────────────────────┐
│          Application Dashboard                │
├───────────────────────────────────────────────┤
│  Requests/sec     Error Rate    Response Time │
│   ┌─────────┐    ┌─────────┐    ┌─────────┐  │
│   │   ╱╲    │    │         │    │     ──  │  │
│   │  ╱  ╲╱╲ │    │ ─────── │    │   ╱╲  ╲ │  │
│   │ ╱      ╲│    │  0.1%   │    │  ╱  ╲─╲ │  │
│   └─────────┘    └─────────┘    └─────────┘  │
│     250/s            ✓ OK        p95: 180ms  │
└───────────────────────────────────────────────┘
```

<v-click>

At a glance:
- Is traffic normal?
- Are errors spiking?
- Is response time degrading?

</v-click>

<!--
Dashboards visualize complex data.
Patterns become visible immediately.
-->

---

# AI and Dashboards

<v-click>

Good news: **AI can generate dashboards for you.**

</v-click>

<v-click>

```
You: "Create a Grafana dashboard showing request rate,
      error rate, and p95 response time for my API,
      with alerts if error rate exceeds 1%."

AI: Here's the dashboard JSON configuration...
```

</v-click>

<v-click>

What used to take hours of manual configuration now takes minutes.

</v-click>

<!--
AI excels at generating configuration.
Dashboard configs are a good use case.
-->

---

# A Debugging Story

**Problem:** Users report "slow sometimes"

<v-clicks>

**Step 1: Check dashboard**
→ Response time spikes to 3000ms at 2pm daily

**Step 2: Check other metrics**
→ CPU at 95%, DB connections maxed at 2pm

**Step 3: Check logs at 2pm**
```
14:00:01 INFO Starting daily report generation
14:00:02 INFO Report query: SELECT * FROM orders...
14:01:45 INFO Report complete (103 seconds)
```

**Step 4: Fix it**
→ Run report at 3am instead, or optimize the query

</v-clicks>

<!--
Without observability, you'd be guessing.
With it, you follow the evidence.
-->

---

# Getting Started

<v-clicks>

**Simple projects: Just use logging**

```python
import logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

logger.info(f"Processing order {order.id}")
```

**Web services: Add basic metrics**
- Request count, error count, response times
- Many frameworks add this automatically

**Production: Add dashboards**
- Set up Grafana or a managed service
- Your mentors can help with setup

</v-clicks>

<!--
Start simple, add complexity as needed.
Logging is always the foundation.
-->

---

# Key Insights

| Concept | Implication |
|---------|-------------|
| Logs tell the story | Text records of what happened |
| Metrics show trends | Numbers over time reveal patterns |
| Dashboards visualize | Graphs make data comprehensible |
| Percentiles reveal suffering | Averages hide outlier pain |
| AI accelerates setup | Configs that took hours now take minutes |

<!--
These are the takeaways.
Observability is about visibility.
-->

---

# Reflection Questions

<v-clicks>

1. You see this log: `ERROR: Something went wrong`. How useful is it? What would make it better?

2. Average response time is 100ms but users complain about slowness. What metric should you check?

3. Why might you want different log levels for development vs. production?

</v-clicks>

<!--
Q1: Useless - needs context, IDs, specific error
Q2: p95 or p99 percentiles
Q3: DEBUG too verbose for production; useful when investigating
-->

---
layout: center
class: text-center
---

# Module 11 Complete

You now understand how to see what your code is doing.

<div class="mt-8 text-xl text-gray-400">
Next: Module 12 — Iterative Design
</div>

<div class="mt-8">
  <a href="./12/" class="px-4 py-2 bg-blue-600 text-white rounded">
    Continue to Module 12 →
  </a>
</div>

<!--
Students understand the three pillars.
Next: why shipping beats perfecting.
-->
